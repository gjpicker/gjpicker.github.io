Welcome to the YanTeam wiki!

## Table of contents
* [深度学习序言](#neural-networks-and-deep-learning)
   * [Course 大纲](#course-summary)
   * [deep learning简介](#introduction-to-deep-learning)
      * [神经元](#what-is-a-neural-network-nn)
      * [机器学习 vs 深度学习](#supervised-learning-with-neural-networks)
      * [为什么 deep learning 有质飞跃?](#why-is-deep-learning-taking-off)


* [深度学习核心](#neural-networks-and-deep-learning)

   * [基础点Basics](#neural-networks-basics)
      * [代价函数Cost function](#logistic-regression-cost-function)
      * [分类 Classification](#binary-classification)
         * [二值与多值 Binary/multi](#binary-classification)
      * [回归 Regression](#logistic-regression)
         * [Softmax](#binary-classification)
         * [Logistic/Linear](#binary-classification)
         * [eg: Cross-entropy](#binary-classification)
     * [激活 active function](#logistic-regression-cost-function)
          * [线性与非线性 Linear vs non-Linear,Why do you need non-linear activation functions?](#binary-classification)
          * [原理： Derivatives of activation functions](#derivatives-of-activation-functions)
          * [举例：Sigmod](#binary-classification)
          * [举例：RELU & *RELU](#binary-classification)
      * [剃度 Gradient Descent](#gradient-descent)
          * [原理: Derivatives & Chain rule](#derivatives)
          * [反相传播推导 Modular backprop & automatic differentiation](#derivatives)
          
		   * [优化：Optimization Algorithms](#neural-networks-basics)
		      * [算法原理：Algorithms](#random-initialization)
		      * [经典算法: Gradient Descent and Stochastic GradientDescent](#parameters-vs-hyperparameters)
		      * [进阶算法: Mini-Batch Stochastic GradientDescent](#random-initialization)
		      * [有记忆力的进阶算法: Momentum](#random-initialization)
		      * [举例：RMSProp](#random-initialization)
		      * [举例：Adam](#random-initialization)


      * [Layers](#computation-graph)
          * [layer概念：Neural Networks Overview](#neural-networks-overview)
          * [网络深度：Shallow/Deep](#derivatives)
          * [原理与推导：Forward and Backward Propagation](#derivatives)
      * [速度、精度、体度可否兼得？Examples](#more-derivatives-examples)
   * [工程：Deep learning train](#neural-networks-basics)
      * [参数、权重、超参 Parameters vs Hyperparameters](#parameters-vs-hyperparameters)
      * [泛化：Regularizing](#random-initialization)
         * [如何开始：Random Initialization](#random-initialization)
         * [如何准备：Normalization](#random-initialization)
         * [如何停止：Generation](#random-initialization)
      * [性能：Efficience](#random-initialization)
         * [标准：train/eval/test](#random-initialization)
         * [回显：EORROr meter](#random-initialization)
         * [Visualization](#random-initialization)
      * [Data](#random-initialization)
         * [数据增强：1KB的数据炼出1GB数据效果。Data augment](#random-initialization)
         * [独立同分布重要性 i.i.d](#random-initialization)
         * [0数据的白手起家：Zero/One shooting learning](#random-initialization)



	   * [如何选择顺手的工具？ Deep learning frameworks](#neural-networks-basics)
	      * [Tensorflow/Keras](#derivatives-with-a-computation-graph)
	      * [Pytorch](#derivatives-with-a-computation-graph)
	      * [PaddlePaddle/ Mxnet/CNTK/Caffe ](#derivatives-with-a-computation-graph)


* [深度学习模块](#neural-networks-and-deep-learning)

   * [卷积CNN](#neural-networks-basics)
      * [图像语意理解， Convolutions for Images](#parameters-vs-hyperparameters)
      * [架构具有理解能力CNN Architecture](#derivatives-with-a-computation-graph)
          * [Alexnet](#logistic-regression-gradient-descent)
          * [VGG](#logistic-regression-gradient-descent)
          * [Inception net](#logistic-regression-gradient-descent)
          * [Resnet/Resnext/Desenet](#logistic-regression-gradient-descent)
      * [推导与原理 Understanding and Visualizing CNN](#derivatives-with-a-computation-graph)
      * [协同工作Transfer Learning and Fine-tuning ](#derivatives-with-a-computation-graph)
      
  * [序列模型 RNN](#neural-networks-basics)
      * [记忆力 Memory](1)
      * [注意力 Attention](1)
      * [应答力 One/Sequence to one/sequence](#gradient-descent-on-m-examples)
      
  * [监督与无监督 Supervised or Unsupervised Learning](#neural-networks-and-deep-learning)
	   * [全监督 Supervised](#course-summary)
	      * [原理与推论](#course-summary)
	      * [数学分析局限性 class labels](#course-summary)
	   * [半监督 Semi-supervised learning ](#course-summary)
	      * [定义 unlabeled data](#course-summary)
	      * [迁移学习 Transductive learning](#course-summary)
	      * [指导学习 Inductive learning](#course-summary)
	      * [意义 Why semi-supervised learning](#course-summary)
	   * [无监督 Unsupervised learning ](#course-summary)
	      * [生成式 Generative Model](#course-summary)
	         * [(化繁為簡) Clustering & Dimension Reduction ](#course-summary)
	         * [(無中生有) Generation ](#course-summary)
	         * [(去蕪存精) Principle Component Analysis (PCA)](#course-summary)
	      * [意义 Why Unsupervised Learning](#table-of-contents)
	      * [数学原理 math of Generative Models](#course-summary)
	         * [逼近 Maximum likelihood](#course-summary)
	         * [变分 Variational methods](#course-summary)
	         * [高流到低流 High dimensional spaces and the manifold hypothesis](#course-summary)
	         * [原理 PCA](#course-summary )
	         * [高斯混合 Gaussian MM](#course-summary)
	      * [无条件 Unconditional models](#course-summary)
	      * [条件 Conditional models](#course-summary)
	      * [不用GAN也可以做到 （略）：VAE](#course-summary)
	          * [原理 why vae ](#course-summary)
	          * [最新技术 Conditional VAE](#course-summary)
	          * [限制 Problems of VAE](#course-summary)
	      * [火热的生成式网络 GAN](#course-summary)
	          * [基本架构 architecture ](#course-summary)
	              * [白帽子 GAN - Discriminator](#course-summary)
	              * [黑帽子 GAN - Generator](#course-summary)
	          * [数学解析 kernel ](#course-summary)
	              * [最优化 optimize difficult ](#course-summary)
	              * [界定最优 GAN与Jensen-Shannon散度 ](#course-summary)
	          * [zoo ](#course-summary)
	              * [LSGAN ](#course-summary)
	              * [wGAN /wGAN-gp ](#course-summary)
	              * [cGAN/cycleGAN ](#course-summary)
	              * [StarGAN ](#course-summary)
	          * [实践 tutorial](#introduction-to-deep-learning)
	
	      
     
* [行业现状 Application](#shallow-neural-networks)
  * [分类 Image-net classify](1)
  * [生成 GAN/style transfer](2)
  * [分割与理解 Auto Drive](3)
  * [梦想 What does this have to do with the brain](#what-does-this-have-to-do-with-the-brain)

* [Summary & Thanks](#shallow-neural-networks)
* [Extra: Ian Goodfellow(xxx) interview](#extra-ian-goodfellow-interview)


