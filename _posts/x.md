Welcome to the YanTeam wiki!

## Table of contents
* [Deap Learning](#neural-networks-and-deep-learning)
   * [Course summary](#course-summary)
   * [Deep Learning introduction](#introduction-to-deep-learning)
      * [Neural-network](#what-is-a-neural-network-nn)
      * [Machine learning vs Deep learning](#supervised-learning-with-neural-networks)
      * [why is deep learning taking off?](#why-is-deep-learning-taking-off)
   * [Deep Learning Basics](#neural-networks-basics)
      * [Cost function](#logistic-regression-cost-function)
      * [Classification](#binary-classification)
         * [Binary/multi](#binary-classification)
      * [Regression](#logistic-regression)
         * [Softmax](#binary-classification)
         * [Logistic/Linear](#binary-classification)
         * [eg: Cross-entropy](#binary-classification)
     * [Active function](#logistic-regression-cost-function)
          * [Linear vs non-Linear,Why do you need non-linear activation functions?](#binary-classification)
          * [Derivatives of activation functions](#derivatives-of-activation-functions)
          * [Sigmod](#binary-classification)
          * [RELU & *RELU](#binary-classification)
      * [Gradient Descent](#gradient-descent)
          * [Derivatives & Chain rule](#derivatives)
          * [Modular backprop & automatic differentiation](#derivatives)
      * [Layers](#computation-graph)
          * [layerï¼šNeural Networks Overview](#neural-networks-overview)
          * [Shallow/Deep](#derivatives)
          * [Forward and Backward Propagation](#derivatives)
      * [Examples](#more-derivatives-examples)
   * [Deep learning train](#neural-networks-basics)
      * [Parameters vs Hyperparameters](#parameters-vs-hyperparameters)
      * [Regularizing](#random-initialization)
         * [Random Initialization](#random-initialization)
         * [Normalization](#random-initialization)
         * [Generation](#random-initialization)
      * [Efficience](#random-initialization)
         * [train/eval/test](#random-initialization)
         * [EORROr meter](#random-initialization)
         * [Visualization](#random-initialization)
      * [Data](#random-initialization)
         * [Data augment](#random-initialization)
         * [i.i.d](#random-initialization)
         * [Zero/One shooting learning](#random-initialization)
   * [Optimization Algorithms](#neural-networks-basics)
      * [Algorithms](#random-initialization)
      * [Gradient Descent and Stochastic GradientDescent](#parameters-vs-hyperparameters)
      * [Mini-Batch Stochastic GradientDescent](#random-initialization)
      * [Momentum](#random-initialization)
      * [RMSProp](#random-initialization)
      * [Adam](#random-initialization)
   * [CNN](#neural-networks-basics)
      * [Convolutions for Images](#parameters-vs-hyperparameters)
      * [CNN Architecture](#derivatives-with-a-computation-graph)
          * [Alexnet](#logistic-regression-gradient-descent)
          * [VGG](#logistic-regression-gradient-descent)
          * [Inception net](#logistic-regression-gradient-descent)
          * [Resnet/Resnext/Desenet](#logistic-regression-gradient-descent)
      * [Understanding and Visualizing CNN](#derivatives-with-a-computation-graph)
      * [Transfer Learning and Fine-tuning ](#derivatives-with-a-computation-graph)
   * [Deep learning frameworks](#neural-networks-basics)
      * [Tensorflow/Keras](#derivatives-with-a-computation-graph)
      * [Pytorch](#derivatives-with-a-computation-graph)
      * [PaddlePaddle/ Mxnet/CNTK/Caffe ](#derivatives-with-a-computation-graph)
  * [RNN](#neural-networks-basics)
      * [Memory](1)
      * [Attention](1)
      * [One/Sequence to one/sequence](#gradient-descent-on-m-examples)
  * [Application](#shallow-neural-networks)
      * [Image-net classify](1)
      * [GAN/style transfer](2)
      * [Auto Drive](3)
      * [What does this have to do with the brain](#what-does-this-have-to-do-with-the-brain)

  * [Summary & Thanks](#shallow-neural-networks)
  * [Extra: Ian Goodfellow(xxx) interview](#extra-ian-goodfellow-interview)


